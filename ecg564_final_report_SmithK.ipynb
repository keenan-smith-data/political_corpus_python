{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Tokenizing Political Language to Determine Bias\"\n",
        "subtitle: \"ECG564 Fall 2022 Project Submission\"\n",
        "author: \"Keenan Smith\"\n",
        "date: \"13 Dec 2022\"\n",
        "abstract: \"\"\n",
        "bibliography: references.bib\n",
        "format:\n",
        "  pdf:\n",
        "    papersize: letter\n",
        "    linestretch: 1.5\n",
        "    fontsize: 12pt\n",
        "    number-sections: true\n",
        "    documentclass: article\n",
        "    classoption: [titlepage, onecolumn]\n",
        "    geometry:\n",
        "      - top=30mm\n",
        "      - left=20mm\n",
        "    fontfamily: libertinus\n",
        "    colorlinks: true\n",
        "editor_options: \n",
        "  chunk_output_type: inline\n",
        "---"
      ],
      "id": "584c08b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "We currently reside in a time period that is politically charged [@politicalcharge]. It is a time where political norms that have been established since at least the 1990s if not earlier are currently in flux. This leads to questions within society of why this moment has arisen and what can be done about it to ease tensions or to incite further change. This is also happening at a time where we are all increasingly connected and data is being created in record numbers. According to the site TechJury, in 2021, \"people created 2.5 Quintilian bytes of data every day\" [@data].\n",
        "\n",
        "So where does it leave this project? My goal is not to solve the politically charged moment with data, but instead to add another tool to enhance the debate. The way we use natural language is and has been an incredibly interesting area of study for most of the modern world. It is all around us in our modern world. If you use a virtual assistant, such as Alexa or Google, you are benefiting from speech recognition software and natural language processing (NLP). If you have ever written an email or wrote a text message and used predictive text to help compose your message, you have benefited from NLP systems and data.\n",
        "\n",
        "This project is the first step in the analysis of written political language and whether the words that author's use when writing political speech is indicative of their political bias. This project itself seeks to answer the question of whether written language and the words used within that language can accurately classify text correctly based on their political leanings.\n",
        "\n",
        "## Background\n",
        "\n",
        "This question came to me over the summer of 2022 when the Supreme Court was set to overturn the 50+ year precedent of Roe V. Wade. I had known that the Supreme Court was anything but a political entity [@sc_political] [@sc_insights] , but I had not really looked too much into the court in my past research. I was always interested in the Legislative or Executive branch. However, with the recent news that Roe v Wade was overturned [@dobbs] I wanted to understand a bit more about this third branch of government.\n",
        "\n",
        "The latest legal theory that has some considerable buy-in from the current court is a legal theory called textualism, which \"is a mode of legal interpretation that focuses on the plain meaning of the text of a legal document.\" [@textualism] and another legal theory called originalism which is \"the idea that the meaning of each provision of the United States Constitution becomes fixed at the time of its enactment.\" [@originalism]. I will not give an in-depth history and background here of these two theories, but they do set up my hypothesis that I wanted to test.\n",
        "\n",
        "## The Research Question\n",
        "\n",
        "Now, this is a complicated question, but my overall question and overall aim is to eventually determine, using NLP techniques, if Supreme Court opinions can be classified by using a political sentiment dictionary. For this project, I narrowed the scope to asking the question: **Given 4 sources, 2 Right-wing and 2 Left-wing, and a corpus of articles written on their websites, can you use the language to accurately classify each document as right-wing or left-wing.**\n",
        "\n",
        "I believe that this is an important question for the time we are currently living. I think it is something that many people have an opinion on if they are interested or even not interested in politics. I think that most people would have some opinion that the words people use give some insight into what political leaning that person may be. I also think that this is shown in political punditry as well as political comedy. There are numerous \"memes\" on the internet making fun of \"insert stereotypical political stance here.\" What I wanted to do was to test this hypothesis. To actually look at the data and see if there is a pattern to which we use speech politically and if there was a corpus that could be defined as right-wing words or left-wing words.\n",
        "\n",
        "# Methods\n",
        "\n",
        "## Data Collection\n",
        "\n",
        "For this project, I will be using a web-scraped data set of article text from four sources.\n",
        "\n",
        "-   Brookings Institute: A nonprofit think-tank based out of DC. They are considered to be a leading progressive think-tank.\n",
        "\n",
        "-   Jacobin Mag: A for profit left-wing news source.\n",
        "\n",
        "-   Heritage Foundation: A nonprofit think-tank based out of DC. They are considered to be a leading conservative think-tank\n",
        "\n",
        "-   The American Mind publication: A magazine distributed and funded by the Claremont Institute, which is a nonprofit think-tank based out of Upland, CA. They are considered to be a leading conservative think-tank.\n",
        "\n",
        "These sources were selected using a ranking of influential political think-tanks [@thinktank] as well as personal interpretation. For instance, Jacobin was included since they represent a truly left-wing stance on American politics compared to the standard news sources.\n",
        "\n",
        "These data were already collected prior to this project over the course of the summer. It was collected using the programming language R, and primarily using the `rvest` package, which is based off the popular `beautifulsoup` library in Python.\n",
        "\n",
        "The full corpus constitutes a collection of 21,756 articles spanning these sources. There are 3,110 Brookings Institute articles. There are 7,205 Jacobin articles. There are 10,582 Heritage Foundation articles. There are 860 American Mind articles. There are a total of 10,315 left-wing and 11442 right-wing articles. You will notice that if you add these articles up, there is one missing. It was written in a foreign language.\n",
        "\n",
        "### Data Limitations\n",
        "\n",
        "These data were collected first using the LinkChecker api to collect the sitemap link data. They were then cleaned and distilled down to articles with text to be scraped. In addition to the text of the article being collected, the title, date, author(s), and, categories (as defined by the publication) were collected. These do not factor into the modeling analysis.\n",
        "\n",
        "It is important to note, that though the date of the article was collected, the articles were not filtered based on date. It is known that language and it's use does change over time. Since the scope of the question being answered in this project is not how political language changes over time, all data were incorporated into this project.\n",
        "\n",
        "## Modelling\n",
        "\n",
        "### Programming Language Choice\n",
        "\n",
        "For this project, I choose to utilize Python as the language of data cleaning, data processing, and data modelling. I am normally an R programmer, but decided that this would be an opportunity to use the Python ecosystem to grow my ability and knowledge in this platform. The drawbacks to this approach are denoted below:\n",
        "\n",
        "-   Scikit-learn has a focus on Machine Learning but not Statistical Output. This means that it uses metrics to determine that the model is a good fit or a poor fit, but it does not have the same statistical summary ability that R has where the statistical models can really be interrogated.[@smltar]\n",
        "\n",
        "-   My ability with Python's visualization libraries are inherently absent. This means that this report will lack in depth visualization, but as this is not the focus of the project, I think this is acceptable. I will grow this skill, but where R has statistical plotting based in the base language, Python takes much more effort for the same results.\n",
        "\n",
        "I would like it to be known that these are my personal issues with the Python machine learning interface and it was a choice that I made to grow my experience in the Python Data Science space. I do not think this affected my ability to do this project well for the scope that I have set for it.\n",
        "\n",
        "### Modelling Methodology\n",
        "\n",
        "For this question, I utilized guidance from Supervised Machine Learning for Text Analysis in R [@smltar] and the Natural Language Toolkit book [@nltk] as well as the Scikit-Learn's documentation, particularly \"Working with Text Data\" [@workingwithtext] and \"Pipelining: chaining a PCA and a Logistic Regression\" [@pipelining].\n",
        "\n",
        "The main objective when working with text data is to get it tokenized. A token can be multiple things in NLP, but for this project, I focused on word tokenization. This is a non-trivial objective with how the text data was collected. The text data had to be combined so that it could be: One Row = One Article. The next challenge was tokenizing these articles where the data could still be extracted out of the Scikit-learn object attributes. This meant that the data needed to be tokenized within the DataFrame object.\n",
        "\n",
        "#### Model Pipeline\n",
        "\n",
        "-   Tokenization\n",
        "\n",
        "-   Lemmatization\n",
        "\n",
        "-   Vectorization\n",
        "\n",
        "-   Hyperparameter Tuning\n",
        "\n",
        "-   Final parameter selection\n",
        "\n",
        "-   Modeling\n",
        "\n",
        "#### Tokenization\n",
        "\n",
        "In this modelling execise, I choose to tokenize the words in each article and then tokenize them into their Lemmas. Lemmatization is a linguistics-based stemming of words. This essentially uses a rules-based approach to tokenizing the word down to its meaning in context and based on linguistics rules. [@smltar] For this, I utilized the tokenizer and the lemmatization processes within the NLTK package in Python. [@nltk]. I choose this methodology because in testing out my models originally, I found that there were a lot of words that would contribute to the modelling score that were just different spellings of the same word. I wanted to have as many unique words be a part of the selection corpus as I reasonably could and Lemmatization was the right course of action with this corpus. I also choose to remove common English stop-words using the NLTK's stopword list.\n",
        "\n",
        "#### Vectorization\n",
        "\n",
        "The last thing that I had to decided before moving on to the modelling portion of the project was how to vectorize the word tokens. There are a few different ways to vectorize natural language including term frequency (TF) or one hot encoding, but since the core question is whether or not the model can classify based on the text to use the Term Frequency-Inverse Document Frequency vectorization method that has long been used in token vectorization.\n",
        "\n",
        "Term Frequency is defined as how frequently a word appears in a document. The inverse document frequency measures how frequent a word is in a unique to the document context. [@tidytext]\n",
        "\n",
        "$$\n",
        "tfidf = f_{i,j} / \\sum_{i'\\in j} f_{i',j} \\space \\cdot \\space log(\\frac{N}{df_i})\n",
        "$$\n",
        "\n",
        "where $f_{i,j}$ is number of occurences of i in j, $df_i$ is the number of documents containing i, and N is the total number of documents.\n",
        "\n",
        "This is a useful vectorization technique since it ensures that words that are rare but heavily used in one document get higher weights than words that are used through all documents. Once the words are vectorized in this manner, they become a part of a document term matrix which is a sparse matrix. In this case, the matrix was quite large considering how large the corpus are. Since the model is tokenized as part of the pipeline in Scikit-learn, I am not sure how large the sparse matrix is.\n",
        "\n",
        "#### Feature Selection\n",
        "\n",
        "Prior to modeling, since the feature space is incredibly rich. I wanted to narrow it down to a certain number of features. I did this in two ways, first in each of the models, I ran a randomized grid search that measured 200, 300, and 400 tokens and 1 token or a 2 token ngram (which is two words next to each other instead of just a single word). I found this quite often when inspecting scikit-learns documentation as parameters to be optimized. [@scikit-learn]\n",
        "\n",
        "Secondly, I utilized scikit-learn's `TruncatedSVD()` utility in the pipeline to do further dimensionality reduction. I also ran parameter optimization with this technique of selecting (5, 15, 30, 45, 60) components to run each model. I chose this technique at the suggestion of scikit-learn and learned that this is called \"Latent Semantic Analysis.\" This is an unsupervised learning technique that is often used in text data to perform feature reduction. It also is a technique that works on the types of sparse matrices that you get with text data. The overall concept is that it performs a Singular Value Decomposition on the text data which will hopefully decompose the text data into vectors whose data encapsulates words that are closely aligned. [@latent]\n",
        "\n",
        "#### Models\n",
        "\n",
        "The models that were chosen to model this question were:\n",
        "\n",
        "-   LASSO Logistic Regression\n",
        "\n",
        "-   Linear Support Vector Machine\n",
        "\n",
        "-   k-Nearest Neighbors\n",
        "\n",
        "-   Random Forest\n",
        "\n",
        "##### LASSO Logistic Regression\n",
        "\n",
        "The LASSO logistic regression model was used because of its simplicity and its ability to do feature reduction. Since this text model has a lot of features, the ability to determine which features were most important in classifying the document was an important question that I wanted to have an answer for. I also chose it for its simplicity and that I am only classifying between two classes. I used the `LogisticRegression()` function in scikit-learn.\n",
        "\n",
        "The only metric that I chose to tune in this model was lambda or C as it is called in scikit-learns nomenclature.\n",
        "\n",
        "##### Linear Support Vector Machine\n",
        "\n",
        "Support Vector Machines are one of the standard text classification models and almost every source I use in this project makes references to them. Since the data are in a large feature space, it appears to be a good space to perform the hyperplane linear classification that comes with SVMs. [@survey] I used the scikit-learn `LinearSVC()` function and used their regularization methods used within the function. I tried to optimize the regularization technique based on l1 or l2, and then the lambda penalty.\n",
        "\n",
        "##### k-Nearest Neighbors\n",
        "\n",
        "k-Nearest Neighbors is good in large spaces since they are a relatively simple classifier that goes based on distance between features to determine the class. I wanted to use this algorithm since it selects features based on similarity since that addresses the main research question. [@survey] I used the `KNeighborsClassifier` in scikit-learn and optimized based on number of neighbors (2, 3, 4, 5, 6).\n",
        "\n",
        "##### Random Forest\n",
        "\n",
        "I used Random Forests because of their improvement over regular decision trees. I also wanted to ensure as much as possible that the tree didn't overfit the training data so that it had good results on the test data. Random Forests are used often in text classification techniques. Random Forest also utilize the \"divide and conquer\" that are easier to understand. [@survey] I used the `RandomForestClassifier()` from scikit-learn and optimized based on max features being (sqrt, log2), criterion (gini, entropy) and max depth (5-50 as determined by numpy).\n",
        "\n",
        "# Results\n",
        "\n",
        "The following section will go over the results of the 4 models and what their metrics were against the test set. The test set is 20% of the total corpus and is shuffled and stratified on the bias classifier.\n",
        "\n",
        "Models were optimized using a Randomized Grid Search script in Python for each model. This was done to save on computational resources. Each optimization run took 60-120 minutes and output 40 results. Each result was derived using 5-fold cross validation that is baked into the `RandomizedGridSearch()` class.\n",
        "\n",
        "The section has the modelling code as a part of the model to show the code utilized. The overall flow will be standard model using the default parameters of each function in the Pipeline of scikit-learn. Then the optimized Pipeline using the parameters found in the grid search.\n",
        "\n",
        "Pipeline objects were used for their reproducibility across all of the models while only having to change the modeling object.\n",
        "\n",
        "## Overall Summary\n",
        "\n",
        "The optimized models performed nearly identically on their Accuracy scores with the highest accuracy score being 87% for the Random Forest model. Each of the other models achieved an 86% accuracy. Accuracy was used as the chief metric in the analysis since it is more important to get the classification right rather than objectives of the metrics of Precision or Sensitivity. The two classes are of equal importance in their classification. It was not important that one was classified correctly more often than the other.\n",
        "\n",
        "As for hyper-parameter selection, every model choose the model with 400 max features. Every model besides knn chose having a 2 ngram token rather than a single word token. This aligns with the fact that two words will give more information than a single word, but it appears to have more impact as well with political speech. Number of components was selected to be 60 in every situation other than the Linear SVM. I think this allows for the same conclusion as the 2 ngram token, which is that more components will capture more of the variance and capture the most information.\n",
        "\n",
        "The overall conclusion is that traditional machine learning techniques using this method reaches their peak at about 86-87% accuracy which means that there is good evidence that the words chosen from this corpus do a good job in correctly classifying political speech from these sources.\n",
        "\n",
        "## Structure of the Results\n",
        "\n",
        "Preceeding the results is a table of the grid search and the top 5 results.\n",
        "\n",
        "First there will be the standard model without any hyperparameter optimization. Then the optimized result.\n",
        "\n",
        "The output from scikit-learn will be the classification metrics based on test set being measured. The second output is an array of the confusion matrix to give an idea of what\n"
      ],
      "id": "c61c4609"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Main Import Statement\n",
        "# Importing Pandas and Numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Importing All Pre-processing Steps\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "# Importing Metrics Module\n",
        "from sklearn import metrics\n",
        "# Importing Homegrown Functions\n",
        "import scripts.blosc_interface as bi\n",
        "import scripts.corpus_split as cs"
      ],
      "id": "Main-Import-Statement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Importing the Tokenized Corpus\n",
        "# Reading Full Corpus from Blosc Pickle\n",
        "full_corpus = bi.blosc_read(\"./data/tokenized_corpus.dat\")"
      ],
      "id": "Importing-the-Tokenized-Corpus",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Splitting the Data\n",
        "x_train, x_test, y_train, y_test = cs.corpus_split(full_corpus)"
      ],
      "id": "Splitting-the-Data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LASSO Logistic Regression\n"
      ],
      "id": "a701dce8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Importing Logistic Regression Grid Search Results\n",
        "# Reading in Randomized Search Results\n",
        "log_reg_cv_results = bi.blosc_read(\"./data/log_grid_search_result.dat\")\n",
        "log_reg_top5 = log_reg_cv_results.query(\"rank_test_score <= 5\")\n",
        "log_reg_top5"
      ],
      "id": "Importing-Logistic-Regression-Grid-Search-Results",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Initial Log Regression with Non-optimal Parameters\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Loading Up the Same Pipeline as Random Search\n",
        "# Updated Values to Optimized\n",
        "log_clf = Pipeline([\n",
        "            ('vect', CountVectorizer()),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            (\"svd\", TruncatedSVD()),\n",
        "            ('log_clf', LogisticRegression())\n",
        "        ])\n",
        "\n",
        "# Fitting the Optimized Log Reg Model\n",
        "log_clf.fit(x_train, y_train)\n",
        "\n",
        "# Getting Predictions on the testset\n",
        "predicted = log_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)\n",
        "\n",
        "# Printing the results\n",
        "print(metrics.classification_report(y_test, predicted))\n",
        "\n",
        "metrics.confusion_matrix(y_test, predicted)"
      ],
      "id": "Initial-Log-Regression-with-Non-optimal-Parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Optimized Logistic Regression\n",
        "\n",
        "# Loading Up the Same Pipeline as Random Search\n",
        "# Updated Values to Optimized\n",
        "log_opt_clf = Pipeline([\n",
        "            ('vect', CountVectorizer(max_features=400, ngram_range=(1,2))),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            (\"svd\", TruncatedSVD(n_components=60)),\n",
        "            ('log_clf', LogisticRegression(penalty=\"l1\", solver = 'saga', C=21.544347, max_iter=1000)), # this needs to be a different solver for LASSO\n",
        "        ])\n",
        "\n",
        "# Fitting the Optimized Log Reg Model\n",
        "log_opt_clf.fit(x_train, y_train)\n",
        "\n",
        "# Getting Predictions on the testset\n",
        "predicted = log_opt_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)\n",
        "\n",
        "# Printing the results\n",
        "print(metrics.classification_report(y_test, predicted))\n",
        "\n",
        "metrics.confusion_matrix(y_test, predicted)"
      ],
      "id": "Optimized-Logistic-Regression",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear SVM\n"
      ],
      "id": "63227e15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Importing Linear SVM Grid Search Results\n",
        "svc_reg_cv_results = bi.blosc_read(\"./data/svc_grid_search_result.dat\")\n",
        "svc_top5 = svc_reg_cv_results.query(\"rank_test_score <= 5\")\n",
        "svc_top5"
      ],
      "id": "Importing-Linear-SVM-Grid-Search-Results",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Initial Linear SVM with Non-optimal Parameters\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svc_clf = Pipeline([\n",
        "            ('vect', CountVectorizer()),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            (\"svd\", TruncatedSVD()),\n",
        "            ('svc_clf', LinearSVC()),\n",
        "        ])\n",
        "\n",
        "svc_clf.fit(x_train, y_train)\n",
        "\n",
        "predicted = svc_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, predicted))\n",
        "\n",
        "metrics.confusion_matrix(y_test, predicted)"
      ],
      "id": "Initial-Linear-SVM-with-Non-optimal-Parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Optimized Linear SVM\n",
        "svc_opt_clf = Pipeline([\n",
        "            ('vect', CountVectorizer(max_features=400, ngram_range=(1,2))),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            (\"svd\", TruncatedSVD(n_components=45)),\n",
        "            ('svc_clf', LinearSVC(clf_penalty = \"l2\", C = 0.046415)),\n",
        "        ])\n",
        "\n",
        "svc_opt_clf.fit(x_train, y_train)\n",
        "\n",
        "predicted = svc_opt_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, predicted))\n",
        "\n",
        "metrics.confusion_matrix(y_test, predicted)"
      ],
      "id": "Optimized-Linear-SVM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## k-Nearest Neighbors\n"
      ],
      "id": "63e3cdbe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Importing k-Nearest Neighbors Grid Search Results\n",
        "knn_reg_cv_results = bi.blosc_read(\"./data/knn_grid_search_result.dat\")\n",
        "knn_top5 = knn_reg_cv_results.query(\"rank_test_score <= 5\")"
      ],
      "id": "Importing-k-Nearest-Neighbors-Grid-Search-Results",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Initial k-Nearest Neighbors with Non-optimal Parameters\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_clf = Pipeline([\n",
        "        ('vect', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        (\"svd\", TruncatedSVD()),\n",
        "        ('knn_clf', KNeighborsClassifier()),\n",
        "    ])\n",
        "\n",
        "knn_clf.fit(x_train, y_train)\n",
        "\n",
        "predicted = knn_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, predicted))\n",
        "\n",
        "metrics.confusion_matrix(y_test, predicted)"
      ],
      "id": "Initial-k-Nearest-Neighbors-with-Non-optimal-Parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knn_opt_clf = Pipeline([\n",
        "        ('vect', CountVectorizer(max_features=400, ngram_range=(1,1))),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        (\"svd\", TruncatedSVD(n_components=60)),\n",
        "        ('knn_clf', KNeighborsClassifier(n_neighbors = 5)),\n",
        "    ])\n",
        "\n",
        "knn_opt_clf.fit(x_train, y_train)\n",
        "\n",
        "predicted = knn_opt_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, predicted))\n",
        "\n",
        "metrics.confusion_matrix(y_test, predicted)"
      ],
      "id": "4b260382",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest\n"
      ],
      "id": "448887ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Importing Random Forest Grid Search Results\n",
        "forest_cv_results = bi.blosc_read(\"./data/forest_grid_search_result.dat\")\n",
        "forest_top5 = forest_cv_results.query(\"rank_test_score <= 5\")"
      ],
      "id": "Importing-Random-Forest-Grid-Search-Results",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Initial Random Forest with Non-optimal Parameters\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_clf = Pipeline([\n",
        "        ('vect', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        (\"svd\", TruncatedSVD()),\n",
        "        ('forest_clf', RandomForestClassifier()),\n",
        "    ])\n",
        "\n",
        "forest_clf.fit(x_train, y_train)\n",
        "\n",
        "predicted = forest_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, predicted))\n",
        "\n",
        "metrics.confusion_matrix(y_test, predicted)"
      ],
      "id": "Initial-Random-Forest-with-Non-optimal-Parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Optimized Random Forest\n",
        "forest_opt_clf = Pipeline([\n",
        "        ('vect', CountVectorizer(max_features=400, ngram_range=(1,2))),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        (\"svd\", TruncatedSVD(n_components=60)),\n",
        "        ('forest_clf', RandomForestClassifier(max_feature=\"log2\",max_depth=35,\n",
        "        criterion=\"entropy\")),\n",
        "    ])\n",
        "\n",
        "forest_opt_clf.fit(x_train, y_train)\n",
        "\n",
        "predicted = forest_opt_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, predicted))\n",
        "\n",
        "metrics.confusion_matrix(y_test, predicted)"
      ],
      "id": "Optimized-Random-Forest",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}